{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Memecoin active trading strategy on Base\n",
    "\n",
    "- Gaussian process optimiser\n",
    "- Discover parameter combinations\n",
    "- Add `min_volatility_threshold` search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Set up\n",
    "\n",
    "Set up Trading Strategy data client.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T08:48:16.546848Z",
     "start_time": "2024-07-19T08:48:16.464970Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from tradingstrategy.client import Client\n",
    "from tradeexecutor.utils.notebook import setup_charting_and_output, OutputMode\n",
    "\n",
    "client = Client.create_jupyter_client()\n",
    "\n",
    "# Set up drawing charts in interactive vector output mode.\n",
    "# This is slower. See the alternative commented option below.\n",
    "# setup_charting_and_output(OutputMode.interactive)\n",
    "\n",
    "# Set up rendering static PNG images.\n",
    "# This is much faster but disables zoom on any chart.\n",
    "setup_charting_and_output(OutputMode.static, image_format=\"png\", width=1500, height=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Parameters\n",
    "\n",
    "- Collection of parameters used in the calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T08:48:16.558597Z",
     "start_time": "2024-07-19T08:48:16.556375Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from skopt.space import Integer, Real, Categorical\n",
    "\n",
    "from tradingstrategy.chain import ChainId\n",
    "from tradingstrategy.timebucket import TimeBucket\n",
    "from tradeexecutor.strategy.cycle import CycleDuration\n",
    "from tradeexecutor.strategy.parameters import StrategyParameters\n",
    "from tradeexecutor.strategy.default_routing_options import TradeRouting\n",
    "\n",
    "\n",
    "class Parameters:\n",
    "\n",
    "    id = \"35-base-atr-optimiser\"\n",
    "\n",
    "    # We trade 1h candle\n",
    "    candle_time_bucket = TimeBucket.h1\n",
    "    cycle_duration = CycleDuration.cycle_1h\n",
    "    \n",
    "    # Coingecko categories to include\n",
    "    #\n",
    "    # See list here: TODO\n",
    "    #\n",
    "    chain_id = ChainId.base\n",
    "    categories = {\"Meme\"}\n",
    "    exchanges = {\"uniswap-v2\", \"uniswap-v3\"}\n",
    "    \n",
    "    #\n",
    "    # Basket construction and rebalance parameters\n",
    "    #\n",
    "    min_asset_universe = 5  # How many assets we need in the asset universe to start running the index\n",
    "\n",
    "    # How many assets our basket can hold once\n",
    "    max_assets_in_portfolio = 10\n",
    "    allocation = 0.95  # Allocate all cash to volatile pairs\n",
    "    # min_rebalance_trade_threshold_pct = 0.05  # % of portfolio composition must change before triggering rebalacne\n",
    "    individual_rebalance_min_threshold_usd = 75.0  # Don't make buys less than this amount    \n",
    "    per_position_cap_of_pool = 0.01  # Never own more than % of the lit liquidity of the trading pool\n",
    "    max_concentration = 0.33 # How large % can one asset be in a portfolio once\n",
    "    min_portfolio_weight = 0.0050  # Close position / do not open if weight is less than 50 BPS\n",
    "\n",
    "    min_signal_threshold = 0\n",
    "\n",
    "    #macd_fast = Categorical([8, 12, 24, 48, 96, 192, 384, 504])\n",
    "    atr_length = Categorical([12, 24, 48, 96, 144, 192, 384, 504])\n",
    "    atr_fraction = Categorical([1.0, 1.5, 2, 3, 5])\n",
    "    # macd_signal = Categorical([5, 7, 9, 12, 24, 48, 96, 192, 384, 504])\n",
    "    \n",
    "    \n",
    "    #\n",
    "    # Inclusion criteria parameters:\n",
    "    # - We set the length of various indicators used in the inclusion criteria\n",
    "    # - We set minimum thresholds neede to be included in the index to filter out illiquid pairs\n",
    "    #\n",
    "\n",
    "    # For the length of trailing sharpe used in inclusion criteria\n",
    "    trailing_sharpe_bars = pd.Timedelta(\"14d\") // candle_time_bucket.to_timedelta()  # How many bars to use in trailing sharpe indicator\n",
    "    # How many bars to use in volatility indicator\n",
    "    rolling_volume_bars = pd.Timedelta(\"7d\") // candle_time_bucket.to_timedelta()\n",
    "    rolling_liquidity_bars = pd.Timedelta(\"7d\") // candle_time_bucket.to_timedelta()\n",
    "    ewm_span = 200  # How many bars to use in exponential moving average for trailing sharpe smoothing\n",
    "    tvl_ewm_span = 200  # How many bars to use in EWM smoothing of TVLs\n",
    "    min_volume = 50_000   # USD\n",
    "    min_liquidity = 200_000  # USD\n",
    "    min_tvl = 25_000  # USD\n",
    "    min_token_sniffer_score = 30  # Scam filter\n",
    "    rebalance_volalitity_bars = pd.Timedelta(\"7d\") // candle_time_bucket.to_timedelta()\n",
    "\n",
    "    #\n",
    "    # Credit integration\n",
    "    #\n",
    "    use_aave = False\n",
    "    \n",
    "    #\n",
    "    # Backtesting only\n",
    "    #\n",
    "    backtest_start = datetime.datetime(2024, 3, 1)\n",
    "    backtest_end = datetime.datetime(2025, 1, 13)\n",
    "    initial_cash = 10_000\n",
    "    min_trade_count = 50  # Used in optimiser/grid search to filter out invvalid results\n",
    "\n",
    "    #\n",
    "    # Live only\n",
    "    #\n",
    "    routing = TradeRouting.default\n",
    "    required_history_period = datetime.timedelta(days=2*14 + 1)\n",
    "    slippage_tolerance = 0.0060  # 0.6% \n",
    "    assummed_liquidity_when_data_missings = 10_000\n",
    "    \n",
    "\n",
    "parameters = StrategyParameters.from_class(Parameters)  # Convert to AttributedDict to easier typing with dot notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Trading pairs and market data\n",
    "\n",
    "- This creates the strategy universe containing pair metadata and their prices\n",
    "- The universe is \"masked\" by simply selecting pairs on the predefined pairs list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T08:48:18.342247Z",
     "start_time": "2024-07-19T08:48:16.556521Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from tradingstrategy.alternative_data.coingecko import CoingeckoUniverse, categorise_pairs\n",
    "from tradingstrategy.chain import ChainId\n",
    "from tradingstrategy.client import Client\n",
    "from tradingstrategy.pair import PandasPairUniverse\n",
    "from tradingstrategy.utils.token_filter import deduplicate_pairs_by_volume\n",
    "from tradingstrategy.client import Client\n",
    "from tradingstrategy.client import Client\n",
    "\n",
    "from tradeexecutor.strategy.trading_strategy_universe import TradingStrategyUniverse\n",
    "from tradeexecutor.strategy.execution_context import ExecutionContext, notebook_execution_context\n",
    "from tradeexecutor.strategy.universe_model import UniverseOptions\n",
    "from tradeexecutor.strategy.trading_strategy_universe import TradingStrategyUniverse, load_partial_data, OHLCVCandleType\n",
    "from tradeexecutor.strategy.execution_context import ExecutionContext, notebook_execution_context\n",
    "from tradeexecutor.strategy.universe_model import UniverseOptions\n",
    "from tradingstrategy.utils.token_extra_data import filter_scams\n",
    "\n",
    "from tradeexecutor.analysis.pair import display_strategy_universe\n",
    "\n",
    "#: Assets used in routing and buy-and-hold benchmark values for our strategy, but not traded by this strategy.\n",
    "SUPPORTING_PAIRS = [\n",
    "    (ChainId.base, \"uniswap-v2\", \"WETH\", \"USDC\", 0.0030),  \n",
    "    (ChainId.base, \"uniswap-v3\", \"cbBTC\", \"WETH\", 0.0030),    # Only trading since October\n",
    "]\n",
    "\n",
    "\n",
    "# Will be converted to cbBTC/ETH->USDC\n",
    "VOL_PAIR = (ChainId.base, \"uniswap-v2\", \"WETH\", \"USDC\", 0.0030)\n",
    "\n",
    "\n",
    "def create_trading_universe(\n",
    "    timestamp: datetime.datetime,\n",
    "    client: Client,\n",
    "    execution_context: ExecutionContext,\n",
    "    universe_options: UniverseOptions,\n",
    ") -> TradingStrategyUniverse:\n",
    "    \"\"\"Create the trading universe.\n",
    "\n",
    "    - Load Trading Strategy full pairs dataset\n",
    "\n",
    "    - Load built-in Coingecko top 1000 dataset\n",
    "    \n",
    "    - Get all DEX tokens for a certain Coigecko category\n",
    "\n",
    "    - Load OHCLV data for these pairs\n",
    "\n",
    "    - Load also BTC and ETH price data to be used as a benchmark\n",
    "    \"\"\"\n",
    "\n",
    "    chain_id = Parameters.chain_id\n",
    "    categories = Parameters.categories\n",
    "\n",
    "    coingecko_universe = CoingeckoUniverse.load()\n",
    "    print(\"Coingecko universe is\", coingecko_universe)\n",
    "\n",
    "    exchange_universe = client.fetch_exchange_universe()\n",
    "    pairs_df = client.fetch_pair_universe().to_pandas()\n",
    "\n",
    "    # Drop other chains to make the dataset smaller to work with\n",
    "    chain_mask = pairs_df[\"chain_id\"] == Parameters.chain_id.value\n",
    "    pairs_df = pairs_df[chain_mask]\n",
    "\n",
    "    # Pull out our benchmark pairs ids.\n",
    "    # We need to construct pair universe object for the symbolic lookup.\n",
    "    pair_universe = PandasPairUniverse(pairs_df, exchange_universe=exchange_universe)\n",
    "    benchmark_pair_ids = [pair_universe.get_pair_by_human_description(desc).pair_id for desc in SUPPORTING_PAIRS]\n",
    "\n",
    "    # Assign categories to all pairs\n",
    "    category_df = categorise_pairs(coingecko_universe, pairs_df)\n",
    "\n",
    "    # Get all trading pairs that are memecoin, across all coingecko data\n",
    "    mask = category_df[\"category\"].isin(categories)\n",
    "    category_pair_ids = category_df[mask][\"pair_id\"]\n",
    "\n",
    "    our_pair_ids = list(category_pair_ids) + benchmark_pair_ids\n",
    "\n",
    "    # From these pair ids, see what trading pairs we have on Ethereum mainnet\n",
    "    pairs_df = pairs_df[pairs_df[\"pair_id\"].isin(our_pair_ids)]\n",
    "\n",
    "    # Limit by DEX\n",
    "    pairs_df = pairs_df[pairs_df[\"exchange_slug\"].isin(Parameters.exchanges)]\n",
    "\n",
    "    # Never deduplicate supporrting pars\n",
    "    supporting_pairs_df = pairs_df[pairs_df[\"pair_id\"].isin(benchmark_pair_ids)]\n",
    "    \n",
    "    # Deduplicate trading pairs - Choose the best pair with the best volume\n",
    "    deduplicated_df = deduplicate_pairs_by_volume(pairs_df)\n",
    "    pairs_df = pd.concat([deduplicated_df, supporting_pairs_df]).drop_duplicates(subset='pair_id', keep='first')\n",
    "\n",
    "    print(\n",
    "        f\"Total {len(pairs_df)} pairs to trade on {chain_id.name} for categories {categories}\",        \n",
    "    )\n",
    "\n",
    "    # Scam filter using TokenSniffer\n",
    "    pairs_df = filter_scams(pairs_df, client, min_token_sniffer_score=Parameters.min_token_sniffer_score)\n",
    "    pairs_df = pairs_df.sort_values(\"volume\", ascending=False)\n",
    "\n",
    "    uni_v2 = pairs_df.loc[pairs_df[\"exchange_slug\"] == \"uniswap-v2\"]\n",
    "    uni_v3 = pairs_df.loc[pairs_df[\"exchange_slug\"] == \"uniswap-v3\"]\n",
    "    print(f\"Pairs on Uniswap v2: {len(uni_v2)}, Uniswap v3: {len(uni_v3)}\")\n",
    "\n",
    "    dataset = load_partial_data(\n",
    "        client=client,\n",
    "        time_bucket=Parameters.candle_time_bucket,\n",
    "        pairs=pairs_df,\n",
    "        execution_context=execution_context,\n",
    "        universe_options=universe_options,\n",
    "        liquidity=True,\n",
    "        liquidity_time_bucket=TimeBucket.d1,  \n",
    "        liquidity_query_type=OHLCVCandleType.tvl_v2,\n",
    "    )\n",
    "\n",
    "    strategy_universe = TradingStrategyUniverse.create_from_dataset(\n",
    "        dataset,\n",
    "        reserve_asset=\"0x833589fCD6eDb6E08f4c7C32D4f71b54bdA02913\",  # USDC on Base \n",
    "        forward_fill=True,  # We got very gappy data from low liquid DEX coins\n",
    "    )\n",
    "\n",
    "    # Tag benchmark/routing pairs tokens so they can be separated from the rest of the tokens\n",
    "    # for the index construction.\n",
    "    strategy_universe.warm_up_data()\n",
    "    for pair_id in benchmark_pair_ids:\n",
    "        pair = strategy_universe.get_pair_by_id(pair_id)\n",
    "        pair.other_data[\"benchmark\"] = True\n",
    "\n",
    "    return strategy_universe\n",
    "\n",
    "\n",
    "# Suppress Pandas warnings within the context manager\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    \n",
    "    strategy_universe = create_trading_universe(\n",
    "        None,\n",
    "        client,\n",
    "        notebook_execution_context,\n",
    "        UniverseOptions.from_strategy_parameters_class(Parameters, notebook_execution_context)\n",
    "    )\n",
    "\n",
    "display_strategy_universe(strategy_universe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asset to trading pair map\n",
    "\n",
    "- Build a helper map\n",
    "- Because we are operating on trading pairs, not on tokens, which are the base asset of a trading pair, we set up \n",
    "  this map to easily look up the selected trading pair by its symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tradingstrategy.types import TokenSymbol\n",
    "from tradeexecutor.state.identifier import TradingPairIdentifier\n",
    "\n",
    "# Create base token symbol to pair map to help later\n",
    "# Token\n",
    "token_map: dict[TokenSymbol, TradingPairIdentifier] = {p.base.token_symbol: p for p in strategy_universe.iterate_pairs()}\n",
    "\n",
    "# print(f\"Pair count is {strategy_universe.get_pair_count()}\")\n",
    "# for symbol, pair in token_map.items():\n",
    "#    print(f\"{symbol} - #{pair.internal_id}\")\n",
    "\n",
    "# Tokens part of benchmark data, but not the strategy\n",
    "benchmark_pair_ids = [p.internal_id for p in strategy_universe.iterate_pairs() if p.other_data.get(\"benchmark\") is True]\n",
    "category_pair_ids = [p.internal_id for p in strategy_universe.iterate_pairs() if p.other_data.get(\"benchmark\") is not True]\n",
    "\n",
    "print(f\"Token map is {len(token_map)} assets\")\n",
    "print(\"Category trading pairs\", len(category_pair_ids))\n",
    "print(\"Benchmark trading pairs\", len(benchmark_pair_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Indicators\n",
    "\n",
    "- Precalculate indicators used by the strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "import pandas_ta\n",
    "\n",
    "from tradeexecutor.strategy.execution_context import ExecutionContext\n",
    "from tradeexecutor.strategy.pandas_trader.indicator import IndicatorSet, IndicatorSource\n",
    "from tradeexecutor.strategy.parameters import StrategyParameters\n",
    "from tradeexecutor.strategy.trading_strategy_universe import TradingStrategyUniverse\n",
    "from tradeexecutor.strategy.pandas_trader.indicator import calculate_and_load_indicators_inline\n",
    "from tradeexecutor.strategy.pandas_trader.indicator import IndicatorDependencyResolver\n",
    "from tradeexecutor.state.types import USDollarAmount\n",
    "from tradeexecutor.strategy.pandas_trader.indicator_decorator import IndicatorRegistry\n",
    "from tradeexecutor.analysis.indicator import display_indicators\n",
    "\n",
    "\n",
    "indicators = IndicatorRegistry()\n",
    "\n",
    "@indicators.define()\n",
    "def trailing_sharpe(\n",
    "    close: pd.Series, \n",
    "    trailing_sharpe_bars: int\n",
    ") -> pd.Series:\n",
    "    \"\"\"Calculate trailing 30d or so returns / standard deviation.\n",
    "\n",
    "    :param length:\n",
    "        Trailing period. \n",
    "    \n",
    "    :return:\n",
    "        Rolling cumulative returns / rolling standard deviation\n",
    "\n",
    "        Note that this trailing sharpe is not annualised.\n",
    "    \"\"\"\n",
    "    ann_factor = pd.Timedelta(days=365) / Parameters.candle_time_bucket.to_pandas_timedelta()\n",
    "    returns = close.pct_change()\n",
    "    mean_returns = returns.rolling(window=trailing_sharpe_bars).mean()    \n",
    "    vol = returns.rolling(window=trailing_sharpe_bars).std()\n",
    "    return mean_returns / vol * np.sqrt(ann_factor)\n",
    "\n",
    "\n",
    "@indicators.define(dependencies=(trailing_sharpe,), source=IndicatorSource.dependencies_only_per_pair)\n",
    "def trailing_sharpe_ewm(\n",
    "    trailing_sharpe_bars: int,\n",
    "    ewm_span: float,\n",
    "    pair: TradingPairIdentifier,\n",
    "    dependency_resolver: IndicatorDependencyResolver,\n",
    ") -> pd.Series:\n",
    "    \"\"\"Expontentially weighted moving average for Sharpe.\n",
    "\n",
    "    :param ewm_span:\n",
    "        How many bars to consider in the EVM\n",
    "\n",
    "    \"\"\"\n",
    "    trailing_sharpe = dependency_resolver.get_indicator_data(\n",
    "        \"trailing_sharpe\",\n",
    "        pair=pair,\n",
    "        parameters={\"trailing_sharpe_bars\": trailing_sharpe_bars},\n",
    "    )    \n",
    "    ewm = trailing_sharpe.ewm(span=ewm_span)\n",
    "    return ewm.mean()\n",
    "\n",
    "\n",
    "@indicators.define()\n",
    "def volatility(close: pd.Series, rebalance_volalitity_bars: int) -> pd.Series:\n",
    "    \"\"\"Calculate the rolling volatility for rebalancing the index for each decision cycle.\"\"\"\n",
    "    price_diff = close.pct_change()\n",
    "    rolling_std = price_diff.rolling(window=rebalance_volalitity_bars).std()\n",
    "    return rolling_std\n",
    "\n",
    "\n",
    "# @indicators.define()\n",
    "# def rolling_returns(\n",
    "#     close: pd.Series, \n",
    "#     returns_bars: int    \n",
    "# ) -> pd.Series:\n",
    "#     \"\"\"Rolling returns for the signal period.\"\"\"\n",
    "\n",
    "#     def _agg_func(window: pd.Series) -> float:\n",
    "\n",
    "#         if len(window) < 2:\n",
    "#             return 0\n",
    "\n",
    "#         try:\n",
    "#             return (window.iloc[-1] - window.iloc[0]) / window.iloc[0]\n",
    "#         except Exception as e:\n",
    "#             raise # Drop into the debugger here\n",
    "\n",
    "#     returns = close.rolling(window=returns_bars).agg(_agg_func)\n",
    "#     return returns\n",
    "\n",
    "\n",
    "#@indicators.define(source=IndicatorSource.close_price)\n",
    "#def rsi(\n",
    "#    close: pd.Series, \n",
    "#    rsi_length: int,\n",
    "#):\n",
    "#    return pandas_ta.rsi(close, length=rsi_length)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@indicators.define(source=IndicatorSource.ohlcv)\n",
    "def atr(\n",
    "    close: pd.Series, \n",
    "    high: pd.Series, \n",
    "    low: pd.Series, \n",
    "    atr_length: int,\n",
    "\n",
    "):    \n",
    "    empty_series = pd.Series([], index=pd.DatetimeIndex([]))\n",
    "    series = pandas_ta.atr(close=close, high=high, low=low, length=atr_length)   \n",
    "    if series is None:\n",
    "        return empty_series\n",
    "    return series\n",
    "\n",
    "\n",
    "\n",
    "@indicators.define(dependencies=[atr])\n",
    "def signal(\n",
    "    close: pd.Series, \n",
    "    atr_length: int,\n",
    "    atr_fraction: int,\n",
    "    pair: TradingPairIdentifier,\n",
    "    dependency_resolver: IndicatorDependencyResolver\n",
    ") -> pd.Series:\n",
    "    \"\"\"Figure out some indicator to predict the price.\"\"\"\n",
    "\n",
    "\n",
    "    atr = dependency_resolver.get_indicator_data(\n",
    "        \"atr\",\n",
    "        parameters={\n",
    "            \"atr_length\": atr_length,\n",
    "        },\n",
    "        pair=pair,\n",
    "    )\n",
    "\n",
    "    return (close - (atr * atr_fraction)) / atr\n",
    "\n",
    "\n",
    "\n",
    "@indicators.define()\n",
    "def volatility_ewm(close: pd.Series, rebalance_volalitity_bars: int) -> pd.Series:\n",
    "    \"\"\"Calculate the rolling volatility for rebalancing the index for each decision cycle.\"\"\"\n",
    "    # We are operating on 1h candles, 14d window\n",
    "    price_diff = close.pct_change()\n",
    "    rolling_std = price_diff.rolling(window=rebalance_volalitity_bars).std()\n",
    "    ewm = rolling_std.ewm(span=14*8)\n",
    "    return ewm.mean()   \n",
    "\n",
    "\n",
    "@indicators.define()\n",
    "def mean_returns(close: pd.Series, rebalance_volalitity_bars: int) -> pd.Series:\n",
    "    # Descripton: TODO\n",
    "    returns = close.pct_change()\n",
    "    mean_returns = returns.rolling(window=rebalance_volalitity_bars).mean()    \n",
    "    return mean_returns\n",
    "\n",
    "\n",
    "@indicators.define()\n",
    "def rolling_cumulative_volume(volume: pd.Series, rolling_volume_bars: int) -> pd.Series:\n",
    "    \"\"\"Calculate rolling volume of the pair.\n",
    "    \n",
    "    - Used in inclusion criteria\n",
    "    \"\"\"\n",
    "    rolling_volume = volume.rolling(window=rolling_volume_bars).sum()\n",
    "    return rolling_volume\n",
    "\n",
    "\n",
    "@indicators.define()\n",
    "def rolling_liquidity_avg(close: pd.Series, rolling_volume_bars: int) -> pd.Series:\n",
    "    \"\"\"Calculate rolling liquidity average\n",
    "\n",
    "    - This is either TVL or XY liquidity (one sided) depending on the trading pair DEX type\n",
    "    \n",
    "    - Used in inclusion criteria\n",
    "    \"\"\"\n",
    "    rolling_liquidity_close = close.rolling(window=rolling_volume_bars).mean()\n",
    "    return  rolling_liquidity_close\n",
    "\n",
    "    \n",
    "@indicators.define(dependencies=(rolling_cumulative_volume,), source=IndicatorSource.strategy_universe)\n",
    "def volume_inclusion_criteria(   \n",
    "    strategy_universe: TradingStrategyUniverse, \n",
    "    min_volume: USDollarAmount,\n",
    "    rolling_volume_bars: int,\n",
    "    dependency_resolver: IndicatorDependencyResolver,\n",
    ") -> pd.Series:\n",
    "    \"\"\"Calculate pair volume inclusion criteria.\n",
    "\n",
    "    - Avoid including illiquid / broken pairs in the set: Pair is included when it has enough volume \n",
    "\n",
    "    TODO: Add liquidity check later\n",
    "\n",
    "    :return:\n",
    "        Series where each timestamp is a list of pair ids meeting the criteria at that timestamp\n",
    "    \"\"\"\n",
    "\n",
    "    series = dependency_resolver.get_indicator_data_pairs_combined(\n",
    "        rolling_cumulative_volume,\n",
    "        parameters={\"rolling_volume_bars\": rolling_volume_bars},\n",
    "    )\n",
    "\n",
    "    # Get mask for days when the rolling volume meets out criteria\n",
    "    mask = series >= min_volume\n",
    "    mask_true_values_only = mask[mask == True]\n",
    "\n",
    "    # Turn to a series of lists\n",
    "    series = mask_true_values_only.groupby(level='timestamp').apply(lambda x: x.index.get_level_values('pair_id').tolist())\n",
    "    return series\n",
    "\n",
    "\n",
    "@indicators.define(dependencies=(volatility_ewm,), source=IndicatorSource.strategy_universe)\n",
    "def volatility_inclusion_criteria(   \n",
    "    strategy_universe: TradingStrategyUniverse, \n",
    "    rebalance_volalitity_bars: int,\n",
    "    dependency_resolver: IndicatorDependencyResolver,\n",
    ") -> pd.Series:\n",
    "    \"\"\"Calculate volatility inclusion criteria.\n",
    "\n",
    "    - Include pairs that are above our threshold signal\n",
    "\n",
    "    :return:\n",
    "        Series where each timestamp is a list of pair ids meeting the criteria at that timestamp\n",
    "    \"\"\"\n",
    "    \n",
    "    series = dependency_resolver.get_indicator_data_pairs_combined(\n",
    "       volatility_ewm,\n",
    "        parameters={\"rebalance_volalitity_bars\": rebalance_volalitity_bars},\n",
    "    )\n",
    "\n",
    "    threshold_pair = strategy_universe.get_pair_by_human_description(VOL_PAIR)\n",
    "    assert threshold_pair\n",
    "    threshold_signal = dependency_resolver.get_indicator_data(\n",
    "        volatility_ewm,\n",
    "        pair=threshold_pair,\n",
    "        parameters={\"rebalance_volalitity_bars\": rebalance_volalitity_bars},\n",
    "    )\n",
    "\n",
    "    assert threshold_signal is not None, \"No threshold volatility signal for: {threshold_pair}\"\n",
    "\n",
    "    # Get mask for days when the rolling volume meets out criteria,\n",
    "    # and max out the threshold signal if there is\n",
    "    # mask = filtered_series >= threshold_signal\n",
    "    df = series.reset_index()\n",
    "    df2 = df.merge(threshold_signal, on=[\"timestamp\"], suffixes=('_pair', '_reference'))\n",
    "\n",
    "    #         pair_id           timestamp  value_pair  value_reference\n",
    "    # 0       4569519 2024-02-13 16:00:00    0.097836              NaN\n",
    "    # 1       4569519 2024-02-13 17:00:00    0.097773              NaN\n",
    "\n",
    "    high_volatility_rows = df2[df2[\"value_pair\"] >= df2[\"value_reference\"]]\n",
    "\n",
    "    def _get_pair_ids_as_list(rows):\n",
    "        return rows[\"pair_id\"].tolist()\n",
    "    \n",
    "    # Turn to a series of lists\n",
    "    series = high_volatility_rows.groupby(by=['timestamp']).apply(_get_pair_ids_as_list)\n",
    "    assert isinstance(series, pd.Series)\n",
    "    return series\n",
    "\n",
    "\n",
    "@indicators.define(source=IndicatorSource.tvl)\n",
    "def tvl(\n",
    "    close: pd.Series,\n",
    ") -> pd.Series:\n",
    "    \"\"\"Get TVL series for a pair.\n",
    "\n",
    "    - Because TVL data is 1d and we use 1h everywhere else, we need to forward fill\n",
    "\n",
    "    - Use previous hourly close as the value\n",
    "    \"\"\"\n",
    "    return close.resample(\"1h\").ffill()\n",
    "\n",
    "\n",
    "@indicators.define(dependencies=(tvl,), source=IndicatorSource.dependencies_only_per_pair)\n",
    "def tvl_ewm(\n",
    "    pair: TradingPairIdentifier,\n",
    "    tvl_ewm_span: float,\n",
    "    dependency_resolver: IndicatorDependencyResolver,\n",
    ") -> pd.Series:\n",
    "    \"\"\"Get smoothed TVL series for a pair.\n",
    "\n",
    "    - Interpretation: If you set span=5, for example, the ewm function will compute an exponential moving average where the weight of the most recent observation is about 33.3% (since Œ±=2/(5+1)‚âà0.333) and this weight decreases exponentially for older observations.\n",
    "\n",
    "    - We forward fill gaps, so there is no missing data in decide_trades()    \n",
    "\n",
    "    - Currently unused in the strategy itself\n",
    "    \"\"\"\n",
    "    tvl_ff = dependency_resolver.get_indicator_data(\n",
    "        tvl,\n",
    "        pair=pair,\n",
    "    )    \n",
    "    return tvl_ff.ewm(span=tvl_ewm_span).mean()\n",
    "\n",
    "\n",
    "@indicators.define(dependencies=(tvl,), source=IndicatorSource.dependencies_only_universe)\n",
    "def tvl_inclusion_criteria(   \n",
    "    min_tvl: USDollarAmount,\n",
    "    dependency_resolver: IndicatorDependencyResolver,\n",
    ") -> pd.Series:\n",
    "    \"\"\"The pair must have min XX,XXX USD one-sided TVL to be included.\n",
    "\n",
    "    - If the Uniswap pool does not have enough ETH or USDC deposited, skip the pair as a scam\n",
    "\n",
    "    :return:\n",
    "        Series where each timestamp is a list of pair ids meeting the criteria at that timestamp\n",
    "    \"\"\"\n",
    "    \n",
    "    series = dependency_resolver.get_indicator_data_pairs_combined(tvl)\n",
    "    mask = series >= min_tvl\n",
    "    # Turn to a series of lists\n",
    "    mask_true_values_only = mask[mask == True]\n",
    "    series = mask_true_values_only.groupby(level='timestamp').apply(lambda x: x.index.get_level_values('pair_id').tolist())\n",
    "    return series\n",
    "\n",
    "\n",
    "\n",
    "@indicators.define(\n",
    "    source=IndicatorSource.strategy_universe\n",
    ")\n",
    "def trading_availability_criteria(\n",
    "    strategy_universe: TradingStrategyUniverse,\n",
    ") -> pd.Series:\n",
    "    \"\"\"Is pair tradeable at each hour.\n",
    "\n",
    "    - The pair has a price candle at that\n",
    "    - Mitigates very corner case issues that TVL/liquidity data is per-day whileas price data is natively per 1h\n",
    "      and the strategy inclusion criteria may include pair too early hour based on TVL only,\n",
    "      leading to a failed attempt to rebalance in a backtest\n",
    "    - Only relevant for backtesting issues if we make an unlucky trade on the starting date\n",
    "      of trading pair listing\n",
    "\n",
    "    :return:\n",
    "        Series with with index (timestamp) and values (list of pair ids trading at that hour)\n",
    "    \"\"\"\n",
    "    # Trading pair availability is defined if there is a open candle in the index for it.\n",
    "    # Because candle data is forward filled, we should not have any gaps in the index.\n",
    "    candle_series = strategy_universe.data_universe.candles.df[\"open\"]\n",
    "    pairs_per_timestamp = candle_series.groupby(level='timestamp').apply(lambda x: x.index.get_level_values('pair_id').tolist())\n",
    "    return pairs_per_timestamp\n",
    "\n",
    "\n",
    "@indicators.define(\n",
    "    dependencies=[\n",
    "        volume_inclusion_criteria,\n",
    "        volatility_inclusion_criteria,\n",
    "        tvl_inclusion_criteria,\n",
    "        trading_availability_criteria\n",
    "    ],\n",
    "    source=IndicatorSource.strategy_universe\n",
    ")\n",
    "def inclusion_criteria(\n",
    "    strategy_universe: TradingStrategyUniverse,\n",
    "    min_volume: USDollarAmount,\n",
    "    rolling_volume_bars: int,\n",
    "    rebalance_volalitity_bars: int,\n",
    "    min_tvl: USDollarAmount,\n",
    "    dependency_resolver: IndicatorDependencyResolver\n",
    ") -> pd.Series:\n",
    "    \"\"\"Pairs meeting all of our inclusion criteria.\n",
    "\n",
    "    - Give the tradeable pair set for each timestamp\n",
    "\n",
    "    :return:\n",
    "        Series where index is timestamp and each cell is a list of pair ids matching our inclusion criteria at that moment\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter out benchmark pairs like WETH in the tradeable pair set\n",
    "    benchmark_pair_ids = set(strategy_universe.get_pair_by_human_description(desc).internal_id for desc in SUPPORTING_PAIRS)\n",
    "\n",
    "    volatility_series = dependency_resolver.get_indicator_data(\n",
    "        volatility_inclusion_criteria,\n",
    "        parameters={\"rebalance_volalitity_bars\": rebalance_volalitity_bars},\n",
    "    )\n",
    "\n",
    "    volume_series = dependency_resolver.get_indicator_data(\n",
    "        volume_inclusion_criteria,\n",
    "        parameters={\n",
    "            \"min_volume\": min_volume,\n",
    "            \"rolling_volume_bars\": rolling_volume_bars,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    tvl_series = dependency_resolver.get_indicator_data(\n",
    "        tvl_inclusion_criteria,\n",
    "        parameters={\n",
    "            \"min_tvl\": min_tvl,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    trading_availability_series = dependency_resolver.get_indicator_data(trading_availability_criteria)\n",
    "\n",
    "    #\n",
    "    # Process all pair ids as a set and the final inclusion\n",
    "    # criteria is union of all sub-criterias\n",
    "    #\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"tvl_pair_ids\": tvl_series,\n",
    "        \"volume_pair_ids\": volume_series,\n",
    "        \"volatility_pair_ids\": volatility_series,\n",
    "        \"trading_availability_pair_ids\": trading_availability_series,\n",
    "    })\n",
    "\n",
    "    # https://stackoverflow.com/questions/33199193/how-to-fill-dataframe-nan-values-with-empty-list-in-pandas\n",
    "    df = df.fillna(\"\").apply(list)\n",
    "\n",
    "    # Volatility criteria removed so we get truly equally weighted index\n",
    "    # def _combine_criteria(row):\n",
    "    #     final_set = set(row[\"volume_pair_ids\"]) & set(row[\"volatility_pair_ids\"]) & set(row[\"tvl_pair_ids\"])\n",
    "    #     return final_set - benchmark_pair_ids\n",
    "\n",
    "    def _combine_criteria(row):\n",
    "        final_set = set(row[\"volume_pair_ids\"]) & \\\n",
    "                    set(row[\"tvl_pair_ids\"]) & \\\n",
    "                    set(row[\"trading_availability_pair_ids\"])\n",
    "        return final_set - benchmark_pair_ids\n",
    "\n",
    "    union_criteria = df.apply(_combine_criteria, axis=1)\n",
    "\n",
    "    # Inclusion criteria data can be spotty at the beginning when there is only 0 or 1 pairs trading,\n",
    "    # so we need to fill gaps to 0\n",
    "    full_index = pd.date_range(\n",
    "        start=union_criteria.index.min(),\n",
    "        end=union_criteria.index.max(),\n",
    "        freq=Parameters.candle_time_bucket.to_frequency(),\n",
    "    )\n",
    "    reindexed = union_criteria.reindex(full_index, fill_value=[])\n",
    "    return reindexed\n",
    "\n",
    "\n",
    "@indicators.define(dependencies=(volume_inclusion_criteria,), source=IndicatorSource.dependencies_only_universe)\n",
    "def volume_included_pair_count(\n",
    "    min_volume: USDollarAmount,\n",
    "    rolling_volume_bars: int,\n",
    "    dependency_resolver: IndicatorDependencyResolver\n",
    ") -> pd.Series:\n",
    "    series = dependency_resolver.get_indicator_data(\n",
    "        volume_inclusion_criteria,\n",
    "        parameters={\"min_volume\": min_volume, \"rolling_volume_bars\": rolling_volume_bars},\n",
    "    )\n",
    "    return series.apply(len)\n",
    "\n",
    "\n",
    "@indicators.define(dependencies=(volatility_inclusion_criteria,), source=IndicatorSource.dependencies_only_universe)\n",
    "def volatility_included_pair_count(\n",
    "    rebalance_volalitity_bars: int,\n",
    "    dependency_resolver: IndicatorDependencyResolver\n",
    ") -> pd.Series:\n",
    "    \"\"\"Calculate number of pairs in meeting volatility criteria on each timestamp\"\"\"\n",
    "    series = dependency_resolver.get_indicator_data(\n",
    "        volatility_inclusion_criteria,\n",
    "        parameters={\"rebalance_volalitity_bars\": rebalance_volalitity_bars},\n",
    "    )\n",
    "    return series.apply(len)\n",
    "\n",
    "\n",
    "@indicators.define(dependencies=(tvl_inclusion_criteria,), source=IndicatorSource.dependencies_only_universe)\n",
    "def tvl_included_pair_count(\n",
    "        min_tvl: USDollarAmount,\n",
    "        dependency_resolver: IndicatorDependencyResolver\n",
    ") -> pd.Series:\n",
    "    \"\"\"Calculate number of pairs in meeting volatility criteria on each timestamp\"\"\"\n",
    "    series = dependency_resolver.get_indicator_data(\n",
    "        tvl_inclusion_criteria,\n",
    "        parameters={\"min_tvl\": min_tvl},\n",
    "    )\n",
    "    series = series.apply(len)\n",
    "\n",
    "    # TVL data can be spotty at the beginning when there is only 0 or 1 pairs trading,\n",
    "    # so we need to fill gaps to 0\n",
    "    full_index = pd.date_range(\n",
    "        start=series.index.min(),\n",
    "        end=series.index.max(),\n",
    "        freq=Parameters.candle_time_bucket.to_frequency(),\n",
    "    )\n",
    "    # Reindex and fill NaN with zeros\n",
    "    reindexed = series.reindex(full_index, fill_value=0)\n",
    "    return reindexed\n",
    "\n",
    "\n",
    "\n",
    "@indicators.define(dependencies=(inclusion_criteria,), source=IndicatorSource.dependencies_only_universe)\n",
    "def all_criteria_included_pair_count(\n",
    "    min_volume: USDollarAmount,\n",
    "    min_tvl: USDollarAmount,\n",
    "    rolling_volume_bars: int,\n",
    "    rebalance_volalitity_bars: int,\n",
    "    dependency_resolver: IndicatorDependencyResolver\n",
    ") -> pd.Series:\n",
    "    \"\"\"Series where each timestamp is the list of pairs meeting all inclusion criteria.\n",
    "\n",
    "    :return:\n",
    "        Series with pair count for each timestamp\n",
    "    \"\"\"\n",
    "    series = dependency_resolver.get_indicator_data(\n",
    "        \"inclusion_criteria\",\n",
    "        parameters={\n",
    "            \"min_volume\": min_volume, \n",
    "            \"min_tvl\": min_tvl, \n",
    "            \"rolling_volume_bars\": rolling_volume_bars,\n",
    "            \"rebalance_volalitity_bars\": rebalance_volalitity_bars,\n",
    "        },\n",
    "    )\n",
    "    return series.apply(len)\n",
    "\n",
    "\n",
    "@indicators.define(source=IndicatorSource.strategy_universe)\n",
    "def trading_pair_count(\n",
    "    strategy_universe: TradingStrategyUniverse,\n",
    ") -> pd.Series:\n",
    "    \"\"\"Get number of pairs that trade at each timestamp.\n",
    "\n",
    "    - Pair must have had at least one candle before the timestamp to be included\n",
    "\n",
    "    - Exclude benchmarks pairs we do not trade\n",
    "\n",
    "    :return:\n",
    "        Series with pair count for each timestamp\n",
    "    \"\"\"\n",
    "\n",
    "    benchmark_pair_ids = {strategy_universe.get_pair_by_human_description(desc).internal_id for desc in SUPPORTING_PAIRS}\n",
    "\n",
    "    # Get pair_id, timestamp -> timestamp, pair_id index\n",
    "    series = strategy_universe.data_universe.candles.df[\"open\"]    \n",
    "    swap_index = series.index.swaplevel(0, 1)\n",
    "\n",
    "    seen_pairs = set()\n",
    "    seen_data = {}\n",
    "\n",
    "    for timestamp, pair_id in swap_index:\n",
    "        if pair_id in benchmark_pair_ids:\n",
    "            continue\n",
    "        seen_pairs.add(pair_id)\n",
    "        seen_data [timestamp] = len(seen_pairs)\n",
    "\n",
    "    series = pd.Series(seen_data.values(), index=list(seen_data.keys()))\n",
    "    return series\n",
    "\n",
    "\n",
    "@indicators.define(dependencies=(volatility_ewm,), source=IndicatorSource.strategy_universe)\n",
    "def volume_weighted_avg_volatility(\n",
    "    strategy_universe,\n",
    "    rebalance_volalitity_bars: int,\n",
    "    dependency_resolver: IndicatorDependencyResolver\n",
    ") -> pd.Series:\n",
    "    \"\"\"Calculate the volume-weighted volatility for the whole index.\n",
    "\n",
    "    This does not make really sense, but we calculate anyway.\n",
    "\n",
    "    :return:\n",
    "        Series with pair count for each timestamp\n",
    "    \"\"\"\n",
    "\n",
    "    volatility_series = dependency_resolver.get_indicator_data_pairs_combined(\n",
    "        volatility_ewm,\n",
    "        parameters={\"rebalance_volalitity_bars\": rebalance_volalitity_bars},\n",
    "    )\n",
    "\n",
    "    volume_series = strategy_universe.data_universe.candles.df[\"volume\"]\n",
    "\n",
    "    # Create DataFrames for easier manipulation\n",
    "    df = pd.DataFrame({\n",
    "        'volatility': volatility_series,\n",
    "        'volume': volume_series\n",
    "    })\n",
    "        \n",
    "    df['weighted'] = df['volatility'] * df['volume']\n",
    "\n",
    "    # Group by timestamp and calculate weighted average\n",
    "    grouped = df.groupby(level='timestamp')\n",
    "    \n",
    "    volume_weighted_volatility = (\n",
    "        grouped['weighted'].sum() / grouped['volume'].sum()\n",
    "    )\n",
    "    \n",
    "    return volume_weighted_volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtest time range\n",
    "\n",
    "- Choose the backtesting time range\n",
    "- Start when we have enough assets (`Parameters.min_asset_universe`) in our asset universe to form the first basket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first date where the condition is True\n",
    "backtest_start = pd.Timestamp(\"2024-04-01\")\n",
    "backtest_end = Parameters.backtest_end\n",
    "\n",
    "print(f\"Time range is {backtest_start} - {backtest_end}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy algorithm and backtest\n",
    "\n",
    "- Run the backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tradeexecutor.backtest.backtest_runner import run_backtest_inline\n",
    "from tradeexecutor.strategy.alpha_model import AlphaModel\n",
    "from tradeexecutor.state.trade import TradeExecution\n",
    "from tradeexecutor.strategy.pandas_trader.strategy_input import StrategyInput, IndicatorDataNotFoundWithinDataTolerance\n",
    "from tradeexecutor.state.visualisation import PlotKind\n",
    "from tradeexecutor.backtest.backtest_runner import run_backtest_inline\n",
    "from tradeexecutor.strategy.tvl_size_risk import USDTVLSizeRiskModel\n",
    "from tradeexecutor.strategy.weighting import weight_by_1_slash_n, weight_passthrouh, weight_equal\n",
    "from tradeexecutor.utils.dedent import dedent_any\n",
    "\n",
    "\n",
    "def decide_trades(\n",
    "    input: StrategyInput\n",
    ") -> list[TradeExecution]:\n",
    "    \"\"\"For each strategy tick, generate the list of trades.\"\"\"\n",
    "    parameters = input.parameters\n",
    "    position_manager = input.get_position_manager()\n",
    "    state = input.state\n",
    "    timestamp = input.timestamp\n",
    "    indicators = input.indicators\n",
    "    strategy_universe = input.strategy_universe\n",
    "\n",
    "    # Build signals for each pair \n",
    "    alpha_model = AlphaModel(\n",
    "        timestamp,\n",
    "        close_position_weight_epsilon=parameters.min_portfolio_weight,  # 10 BPS is our min portfolio weight\n",
    "    )\n",
    "\n",
    "    # Prepare diagnostics variables\n",
    "    max_vol = (0, None)  \n",
    "    signal_count = 0  \n",
    "    \n",
    "    # Get pairs included in this rebalance cycle.\n",
    "    # This includes pair that have been pre-cleared in inclusion_criteria()\n",
    "    # with volume, volatility and TVL filters \n",
    "    included_pairs = indicators.get_indicator_value(\n",
    "        \"inclusion_criteria\",\n",
    "        na_conversion=False,\n",
    "    )\n",
    "    if included_pairs is None:\n",
    "        included_pairs = []\n",
    "\n",
    "    clipped_pairs = 0\n",
    "\n",
    "    # Set signal for each pair\n",
    "    for pair_id in included_pairs:\n",
    "        pair = strategy_universe.get_pair_by_id(pair_id)\n",
    "\n",
    "        signal = indicators.get_indicator_value(\"signal\", pair=pair)\n",
    "        if signal is None:\n",
    "            continue\n",
    "\n",
    "        weight = signal\n",
    "\n",
    "        if weight < 0:\n",
    "            continue\n",
    "    \n",
    "        alpha_model.set_signal(\n",
    "            pair,\n",
    "            weight,\n",
    "        )\n",
    "\n",
    "        # Diagnostics reporting\n",
    "        signal_count += 1\n",
    "\n",
    "\n",
    "    # Calculate how much dollar value we want each individual position to be on this strategy cycle,\n",
    "    # based on our total available equity\n",
    "    portfolio = position_manager.get_current_portfolio()\n",
    "    portfolio_target_value = portfolio.get_total_equity() * parameters.allocation\n",
    "\n",
    "    # Select max_assets_in_portfolio assets in which we are going to invest\n",
    "    # Calculate a weight for ecah asset in the portfolio using 1/N method based on the raw signal\n",
    "    alpha_model.select_top_signals(count=parameters.max_assets_in_portfolio)\n",
    "    alpha_model.assign_weights(method=weight_passthrouh)\n",
    "    # alpha_model.assign_weights(method=weight_by_1_slash_n)\n",
    "\n",
    "    #\n",
    "    # Normalise weights and cap the positions\n",
    "    # \n",
    "    size_risk_model = USDTVLSizeRiskModel(\n",
    "        pricing_model=input.pricing_model,\n",
    "        per_position_cap=parameters.per_position_cap_of_pool,  # This is how much % by all pool TVL we can allocate for a position\n",
    "        missing_tvl_placeholder_usd=parameters.assummed_liquidity_when_data_missings,  # Placeholder for missing TVL data until we get the data off the chain\n",
    "    )\n",
    "\n",
    "    alpha_model.normalise_weights(\n",
    "        investable_equity=portfolio_target_value,\n",
    "        size_risk_model=size_risk_model,\n",
    "        max_weight=parameters.max_concentration,\n",
    "    )\n",
    "\n",
    "    # Load in old weight for each trading pair signal,\n",
    "    # so we can calculate the adjustment trade size\n",
    "    alpha_model.update_old_weights(\n",
    "        state.portfolio,\n",
    "        ignore_credit=True,\n",
    "    )\n",
    "    alpha_model.calculate_target_positions(position_manager)\n",
    "\n",
    "    # Shift portfolio from current positions to target positions\n",
    "    # determined by the alpha signals (momentum)\n",
    "    \n",
    "    # rebalance_threshold_usd = portfolio_target_value * parameters.min_rebalance_trade_threshold_pct\n",
    "    rebalance_threshold_usd = parameters.individual_rebalance_min_threshold_usd\n",
    "    \n",
    "    assert rebalance_threshold_usd > 0.1, \"Safety check tripped - something like wrong with strat code\"\n",
    "    trades = alpha_model.generate_rebalance_trades_and_triggers(\n",
    "        position_manager,\n",
    "        min_trade_threshold=rebalance_threshold_usd,  # Don't bother with trades under XXXX USD\n",
    "        invidiual_rebalance_min_threshold=parameters.individual_rebalance_min_threshold_usd,\n",
    "        execution_context=input.execution_context,\n",
    "    )\n",
    "\n",
    "    # Supply or withdraw cash to Aave if strategy is set to do so\n",
    "    if parameters.use_aave:\n",
    "        credit_deposit_flow = position_manager.calculate_credit_flow_needed(\n",
    "            trades,\n",
    "            parameters.allocation,\n",
    "        )\n",
    "        trades += position_manager.manage_credit_flow(credit_deposit_flow)\n",
    "    else:\n",
    "        credit_deposit_flow = 0\n",
    "\n",
    "    # Add verbal report about decision made/not made,\n",
    "    # so it is much easier to diagnose live trade execution.\n",
    "    # This will be readable in Discord/Telegram logging.\n",
    "    if input.is_visualisation_enabled():\n",
    "\n",
    "        vol_pair = strategy_universe.get_pair_by_human_description(VOL_PAIR)\n",
    "        volume_included_pair_count = indicators.get_indicator_value(\n",
    "            \"volume_included_pair_count\",\n",
    "        )\n",
    "        volatility_included_pair_count = indicators.get_indicator_value(\n",
    "            \"volatility_included_pair_count\",\n",
    "        ) \n",
    "        tvl_included_pair_count = indicators.get_indicator_value(\n",
    "            \"tvl_included_pair_count\",\n",
    "        )        \n",
    "        \n",
    "        try:\n",
    "            top_signal = next(iter(alpha_model.get_signals_sorted_by_weight()))\n",
    "            if top_signal.normalised_weight == 0:\n",
    "                top_signal = None\n",
    "        except StopIteration:\n",
    "            top_signal = None\n",
    "\n",
    "        max_vol_pair = max_vol[1]\n",
    "        if max_vol_pair:\n",
    "            max_vol_signal = alpha_model.get_signal_by_pair(max_vol_pair)\n",
    "        else:\n",
    "            max_vol_signal = None\n",
    "\n",
    "        vol_pair_vol = indicators.get_indicator_value(\"volatility_ewm\", pair=vol_pair)\n",
    "\n",
    "        rebalance_volume = sum(t.get_value() for t in trades)\n",
    "        \n",
    "        report = dedent_any(f\"\"\"\n",
    "        Cycle: #{input.cycle}\n",
    "        Rebalanced: {'üëç' if alpha_model.is_rebalance_triggered() else 'üëé'}\n",
    "        Open/about to open positions: {len(state.portfolio.open_positions)} \n",
    "        Max position value change: {alpha_model.max_position_adjust_usd:,.2f} USD\n",
    "        Rebalance threshold: {alpha_model.position_adjust_threshold_usd:,.2f} USD\n",
    "        Trades decided: {len(trades)}\n",
    "        Pairs total: {strategy_universe.data_universe.pairs.get_count()}\n",
    "        Pairs meeting inclusion criteria: {len(included_pairs)}\n",
    "        Pairs meeting volume inclusion criteria: {volume_included_pair_count}\n",
    "        Pairs meeting volatility inclusion criteria: {volatility_included_pair_count}        \n",
    "        Pairs meeting TVL inclusion criteria: {tvl_included_pair_count}        \n",
    "        Pairs under volatility threshold: {clipped_pairs}        \n",
    "        Signals created: {signal_count}\n",
    "        Total equity: {portfolio.get_total_equity():,.2f} USD\n",
    "        Cash: {position_manager.get_current_cash():,.2f} USD\n",
    "        Investable equity: {alpha_model.investable_equity:,.2f} USD\n",
    "        Accepted investable equity: {alpha_model.accepted_investable_equity:,.2f} USD\n",
    "        Allocated to signals: {alpha_model.get_allocated_value():,.2f} USD\n",
    "        Discarted allocation because of lack of lit liquidity: {alpha_model.size_risk_discarded_value:,.2f} USD\n",
    "        Credit deposit flow: {credit_deposit_flow:,.2f} USD\n",
    "        Rebalance volume: {rebalance_volume:,.2f} USD\n",
    "        {vol_pair.base.token_symbol} volatility: {vol_pair_vol}        \n",
    "        Most volatility pair: {max_vol_pair.get_ticker() if max_vol_pair else '-'}\n",
    "        Most volatility pair vol: {max_vol[0]}\n",
    "        Most volatility pair signal value: {max_vol_signal.signal if max_vol_signal else '-'}\n",
    "        Most volatility pair signal weight: {max_vol_signal.raw_weight if max_vol_signal else '-'}        \n",
    "        \"\"\")\n",
    "\n",
    "        # Most volatility pair signal weight (normalised): {max_vol_signal.normalised_weight * 100 if max_vol_signal else '-'} % (got {max_vol_signal.position_size_risk.get_relative_capped_amount() * 100 if max_vol_signal else '-'} % of asked size)\n",
    "        if top_signal:\n",
    "            top_signal_vol = indicators.get_indicator_value(\"volatility_ewm\", pair=top_signal.pair)\n",
    "            assert top_signal.position_size_risk\n",
    "            report += dedent_any(f\"\"\"\n",
    "            Top signal pair: {top_signal.pair.get_ticker()}\n",
    "            Top signal volatility: {top_signal_vol}\n",
    "            Top signal value: {top_signal.signal}\n",
    "            Top signal weight: {top_signal.raw_weight}\n",
    "            Top signal weight (normalised): {top_signal.normalised_weight * 100:.2f} % (got {top_signal.position_size_risk.get_relative_capped_amount() * 100:.2f} % of asked size)\n",
    "            \"\"\")            \n",
    "\n",
    "        for flag, count in alpha_model.get_flag_diagnostics_data().items():\n",
    "            report += f\"Signals with flag {flag.name}: {count}\\n\"\n",
    "\n",
    "        state.visualisation.add_message(\n",
    "            timestamp,\n",
    "            report, \n",
    "        )\n",
    "\n",
    "        state.visualisation.set_discardable_data(\"alpha_model\", alpha_model)\n",
    "    \n",
    "    return trades  # Return the list of trades we made in this cycle\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimiser\n",
    "\n",
    "- Run optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from tradeexecutor.backtest.optimiser import perform_optimisation\n",
    "from tradeexecutor.backtest.optimiser import prepare_optimiser_parameters\n",
    "from tradeexecutor.backtest.optimiser import MinTradeCountFilter\n",
    "from tradeexecutor.backtest.optimiser_functions import optimise_sharpe, optimise_sortino\n",
    "\n",
    "# How many Gaussian Process iterations we do\n",
    "iterations = 2\n",
    "\n",
    "# What do we optimise for\n",
    "# search_func = BalancedSharpeAndMaxDrawdownOptimisationFunction(sharpe_weight=0.75, max_drawdown_weight=0.25)\n",
    "search_func = optimise_sortino\n",
    "\n",
    "optimiser_result = perform_optimisation(\n",
    "    iterations=iterations,\n",
    "    search_func=search_func,\n",
    "    decide_trades=decide_trades,\n",
    "    strategy_universe=strategy_universe,\n",
    "    parameters=prepare_optimiser_parameters(Parameters),  # Handle scikit-optimise search space\n",
    "    create_indicators=indicators.create_indicators,\n",
    "    result_filter=MinTradeCountFilter(Parameters.min_trade_count),\n",
    "    timeout=20*60,    \n",
    "    # We are searching wacky parameter combinations and\n",
    "    # some of them lead to buggy strategies,\n",
    "    # by setting ignore_wallet_errors we just zero out buggy\n",
    "    # strategies instead of crashing with an exception\n",
    "    ignore_wallet_errors=True,  \n",
    "    # Uncomment for diagnostics\n",
    "    # log_level=logging.INFO,\n",
    "    # max_workers=1,\n",
    ")\n",
    "\n",
    "print(f\"Optimise completed, optimiser searched {optimiser_result.get_combination_count()} combinations, with {optimiser_result.get_cached_count()} results read directly from cache. and {optimiser_result.get_filtered_count()} filtered results.\")\n",
    "print(\"Backtests failed with exception:\", optimiser_result.get_failed_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "- Show the top results of all optimiser iterations in a single table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tradeexecutor.analysis.optimiser import analyse_optimiser_result\n",
    "from tradeexecutor.analysis.grid_search import render_grid_search_result_table\n",
    "\n",
    "filtered = [r for r in optimiser_result.results if r.filtered]\n",
    "print(f\"Filtering out {len(filtered)} results\")\n",
    "\n",
    "# Optimiser already filtered for min_positions_threshold when doing the optimiser run\n",
    "df = analyse_optimiser_result(\n",
    "    optimiser_result,\n",
    "    max_search_results=300,\n",
    ")\n",
    "print(f\"Showing the best {len(df)} results\")\n",
    "\n",
    "\n",
    "render_grid_search_result_table(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Render the candidate with the best equity curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tradeexecutor.visual.grid_search import visualise_single_grid_search_result_benchmark\n",
    "\n",
    "# GridSearchResult instance that gave the best performance\n",
    "best_pick = optimiser_result.results[0].result\n",
    "state = best_pick.hydrate_state()\n",
    "\n",
    "print(f\"The best result found for {search_func} was {best_pick}\")\n",
    "\n",
    "fig = visualise_single_grid_search_result_benchmark(\n",
    "    best_pick, \n",
    "    strategy_universe, \n",
    "    initial_cash=Parameters.initial_cash,\n",
    "    log_y=True,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio performance (best pick)\n",
    "\n",
    "- Compare buy and hold against our best performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tradeexecutor.analysis.multi_asset_benchmark import compare_strategy_backtest_to_multiple_assets\n",
    "\n",
    "returns = best_pick.returns\n",
    "\n",
    "df = compare_strategy_backtest_to_multiple_assets(\n",
    "    state=state,\n",
    "    strategy_universe=strategy_universe,\n",
    "    returns=returns,\n",
    "    display=True,\n",
    "    asset_count=3,\n",
    ")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trade summary (best pick)\n",
    "\n",
    "- Show statistics about the made trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = best_pick.summary\n",
    "display(summary.to_dataframe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trading pair performance breakdown\n",
    "\n",
    "- Show breakdown of different pairs on the best result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tradeexecutor.analysis.multipair import analyse_multipair\n",
    "from tradeexecutor.analysis.multipair import format_multipair_summary\n",
    "\n",
    "multipair_summary = analyse_multipair(state)\n",
    "display(format_multipair_summary(multipair_summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best positions\n",
    "\n",
    "- Find positions that made most profit for the leading backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profit_by_position = [(p.get_realised_profit_usd(), p) for p in state.portfolio.get_all_positions()]\n",
    "profit_by_position.sort(key=lambda t: t[0] or -999_999_999, reverse=True)\n",
    "\n",
    "data = []\n",
    "\n",
    "for profit, p in profit_by_position[0:5]:\n",
    "    data.append({\n",
    "        \"Profit USD\": profit,\n",
    "        \"Pair\": p.pair.get_ticker(),\n",
    "        \"Position id\": p.position_id,\n",
    "        \"Opened\": p.opened_at,\n",
    "        \"Closed\": p.closed_at,\n",
    "        \"Duration\": p.closed_at - p.opened_at if p.closed_at else \"(still open)\",\n",
    "        \"Open price\": p.get_opening_price(),\n",
    "        \"Close price\": p.get_closing_price(),\n",
    "        \"Trades\": p.get_trade_count(),\n",
    "        \"Price gain %\": (p.get_closing_price() - p.get_opening_price()) / p.get_opening_price() * 100,\n",
    "        \"Weight at open %\": p.get_capital_tied_at_open_pct() * 100,\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rolling sharpe\n",
    "\n",
    "- The rolling sharpe ratio of the best pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "from tradeexecutor.visual.equity_curve import calculate_equity_curve, calculate_returns\n",
    "from tradeexecutor.visual.equity_curve import calculate_rolling_sharpe\n",
    "\n",
    "rolling_sharpe = calculate_rolling_sharpe(\n",
    "    returns,\n",
    "    freq=\"D\",\n",
    "    periods=180,\n",
    ")\n",
    "\n",
    "fig = px.line(rolling_sharpe, title='Strategy rolling Sharpe (6 months)')\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.update_yaxes(title=\"Sharpe\")\n",
    "fig.update_xaxes(title=\"Time\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data diagnosics\n",
    "\n",
    "- Some example code to track down issues with data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = [{\"Timestamp\": s.calculated_at, \"Equity\": s.net_asset_value} for s in state.stats.portfolio]\n",
    "# df = pd.DataFrame(data)\n",
    "# df = df.set_index(\"Timestamp\")\n",
    "\n",
    "# with pd.option_context('display.min_rows', 50, \"display.max_rows\", 50):\n",
    "#     display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for t in state.portfolio.get_all_trades():\n",
    "#     if t.executed_at.date() == datetime.date(2023, 12, 29):\n",
    "#         print(t, t.pair, t.get_value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go\n",
    "\n",
    "# pair_desc = (ChainId.ethereum, \"uniswap-v3\", \"WBTC\", \"USDC\", 0.0005)\n",
    "# pair = strategy_universe.get_pair_by_human_description(pair_desc)\n",
    "# df = strategy_universe.data_universe.candles.get_candles_by_pair(pair.internal_id)\n",
    "\n",
    "\n",
    "# fig = go.Figure(data=go.Ohlc(x=df.timestamp,\n",
    "#                     open=df.open,\n",
    "#                     high=df.high,\n",
    "#                     low=df.low,\n",
    "#                     close=df.close))\n",
    "# fig.show()\n",
    "\n",
    "# with pd.option_context('display.min_rows', 5000, \"display.max_rows\", 5000, 'display.float_format', lambda x: f'{x:.6f}'):\n",
    "#     display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trading-strategy-getting-started-CRdZaTBS-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
